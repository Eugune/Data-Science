{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3398f8a0",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e9080",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7f82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b8dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01841f91",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e41e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f73c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "\n",
    "# create a variable that number of data need to split from the train dataset.\n",
    "# In general, 7:2:1 or 8:1:1\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "\n",
    "# Create a scaler to scale our data since we prefer to have inputs between 0 and 1\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "# scale oure train and validation data,  test data respectively\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "\n",
    "# We need to shuffle the data to make it random so that each batch is representative. \n",
    "# To optimize computation, we set the size of the shuffled batch to handle it at once until the entire dataset is shuffled.\n",
    "BUFFER_SIZE =10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# Split the validation dataset from train dataset.\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "\n",
    "# We use mini batch method and iterate over the different batches\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test dataset (formating_batch, size remain)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation__inputs, validation_tragets = next(iter(validation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e0f3c",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "    building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5395b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = 28*28\n",
    "# output = 10 numbers\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "# Use same hidden layer size for both hidden layers\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # flatten means you need to change it 1D array as inputs\n",
    "    tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output dot(input, weight) + bias\n",
    "    \n",
    "    # 1st  hidden layers\n",
    "    # ReLU =  A rectified linear unit (ReLU)\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    \n",
    "    # 2nt  hidden layers\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    \n",
    "    # the final output layer\n",
    "    # softmax = Softmax activation\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "        \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a0af2",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a234cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimization process involves updating the model parameters (weights and biases)\n",
    "# ---based on the gradients of the loss function with respect to those parameters\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d44648",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d1d39d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.2731 - accuracy: 0.9198 - val_loss: 0.1231 - val_accuracy: 0.9623 - 3s/epoch - 5ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 1s - loss: 0.1043 - accuracy: 0.9678 - val_loss: 0.0900 - val_accuracy: 0.9727 - 1s/epoch - 3ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 1s - loss: 0.0717 - accuracy: 0.9779 - val_loss: 0.0590 - val_accuracy: 0.9815 - 1s/epoch - 3ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 1s - loss: 0.0504 - accuracy: 0.9839 - val_loss: 0.0529 - val_accuracy: 0.9837 - 1s/epoch - 3ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 1s - loss: 0.0405 - accuracy: 0.9865 - val_loss: 0.0407 - val_accuracy: 0.9865 - 1s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x261be2d51f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# fit the model\n",
    "# \n",
    "model.fit(train_data, epochs= NUM_EPOCHS, validation_data=(validation__inputs,validation_tragets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad29fa",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "    after training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bfde3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 364ms/step - loss: 0.0724 - accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1d0547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07. Test accuracy: 97.78%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7331c7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
