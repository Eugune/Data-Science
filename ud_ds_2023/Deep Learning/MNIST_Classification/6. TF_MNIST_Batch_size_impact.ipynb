{"cells":[{"cell_type":"markdown","metadata":{"id":"MAEUtjdl2I18"},"source":["# Exercises\n","\n","### 6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n","\n","**Solution**\n","\n","Find the line that declares the batch size.\n","\n","Change batch_size from 100 to 10000.\n","\n","    BATCH_SIZE = 10000\n","    \n","A bigger batch size results in slower training. That's what we expected from the theory. We are taking advantage of batching because of the amazing speed increase.\n","\n","Notice that the validation accuracy starts from a low number and with 5 epochs actually **finishes** at a lower number. That's because there are **fewer** updates in a single epoch.\n","\n","*Try a batch size of 30,000 or 50,000. That's very close to single batch GD for this problem. What do you think about the speed?You will need to change the max epochs to 100 (for instance), as 5 epochs won't be enough to train the model. What do you think about the speed of optimization?*"]},{"cell_type":"markdown","source":["Change batch_size from 100 to 1.\n","\n","batch_size = 1\n","\n","**A batch size of 1 results in the SGD. It takes the algorithm very little time to process a single batch (as it is one data point**), but there are thousands of batches (**54000 to be precise**), thus the algorithm is actually slow. Remember that this depends on the number of cores that you train on. If you are using a CPU with 4 or 8 cores, you can only train 4 or 8 batches at once. The middle ground (mini-batching such as 100 samples per batch) is optimal.\n","\n","Notice that the validation accuracy starts from a high number. That's because there are lots updates in a single epoch. Once the training is over, the accuracy is lower than all other batch sizes (SGD was an approximation)."],"metadata":{"id":"WCBlS1kd-Kj9"}},{"cell_type":"markdown","source":["\n","\n","1.   2 hidden layers, 50 layers size, act func 'relu', 'relu', batch size 100  \n","    Epoch 1/5 13s loss: 0.4045 accuracy: 0.8868 val_accuracy: 0.9408  \n","    Epoch 5/5 3s loss: 0.0910 accuracy: 0.9732 val_accuracy: 0.9717  \n","2.   2 hidden layers, 50 layers size, act func 'relu', 'relu', batch size 10000  \n","    Epoch 1/5 10s loss: 2.2362 accuracy: 0.1756 val_accuracy: 0.3723  \n","    Epoch 5/5 3s loss: 1.0924 accuracy: 0.7246 val_accuracy: 0.7882  \n","3.   2 hidden layers, 50 layers size, act func 'relu', 'relu', batch size 1  \n","    Epoch 1/5 168s loss: 0.2508 accuracy: 0.9252 val_accuracy: 0.9522  \n","    Epoch 5/5 139s loss: 0.1329 accuracy: 0.9672 val_accuracy: 0.9630  \n","\n","\n","\n"],"metadata":{"id":"Wz3i9hML5V3W"}},{"cell_type":"markdown","metadata":{"id":"A2m7oCM-2I1-"},"source":["# Deep Neural Network for MNIST Classification"]},{"cell_type":"markdown","metadata":{"id":"QYKoYdQ22I1_"},"source":["## Import the relevant packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"M8e21a1o2I1_","executionInfo":{"status":"ok","timestamp":1701058205241,"user_tz":300,"elapsed":4546,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n"]},{"cell_type":"markdown","metadata":{"id":"9F2aFVxQ2I2A"},"source":["## Data\n","\n","That's where we load and preprocess our data."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dy14I-HK2I2A","executionInfo":{"status":"ok","timestamp":1701058207170,"user_tz":300,"elapsed":1932,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["\n","mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n","\n","mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n","\n","num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n","num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n","\n","num_test_samples = mnist_info.splits['test'].num_examples\n","num_test_samples = tf.cast(num_test_samples, tf.int64)\n","\n","\n","def scale(image, label):\n","    image = tf.cast(image, tf.float32)\n","    image /= 255.\n","\n","    return image, label\n","\n","scaled_train_and_validation_data = mnist_train.map(scale)\n","test_data = mnist_test.map(scale)\n","\n","\n","BUFFER_SIZE = 10000\n","\n","shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n","validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n","train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n","\n","\n","BATCH_SIZE = 1\n","\n","train_data = train_data.batch(BATCH_SIZE)\n","validation_data = validation_data.batch(num_validation_samples)\n","test_data = test_data.batch(num_test_samples)\n","\n","validation_inputs, validation_targets = next(iter(validation_data))"]},{"cell_type":"markdown","metadata":{"id":"OXrUCdDF2I2A"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"1HAp6c4h2I2B"},"source":["### Outline the model\n","When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dEMb_HkB2I2B","executionInfo":{"status":"ok","timestamp":1701058207171,"user_tz":300,"elapsed":10,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["input_size = 784\n","output_size = 10\n","\n","hidden_layer_size = 50\n","\n","model = tf.keras.Sequential([\n","\n","    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n","\n","    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n","    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n","    # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n","    # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n","    # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 5th hidden layer\n","\n","    # the final layer is no different, we just make sure to activate it with softmax\n","    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n","])"]},{"cell_type":"markdown","metadata":{"id":"7rIiKhOZ2I2B"},"source":["### Choose the optimizer and the loss function"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"V8mfIaMn2I2B","executionInfo":{"status":"ok","timestamp":1701058207171,"user_tz":300,"elapsed":9,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"7igwa6At2I2B"},"source":["### Training\n","That's where we train the model we have built."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcrUwEij2I2B","executionInfo":{"status":"ok","timestamp":1701058939647,"user_tz":300,"elapsed":732484,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}},"outputId":"f27c1899-1d4b-4f93-e312-4e378e338118"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","54000/54000 - 168s - loss: 0.2508 - accuracy: 0.9252 - val_loss: 0.1722 - val_accuracy: 0.9522 - 168s/epoch - 3ms/step\n","Epoch 2/5\n","54000/54000 - 141s - loss: 0.1589 - accuracy: 0.9543 - val_loss: 0.1421 - val_accuracy: 0.9593 - 141s/epoch - 3ms/step\n","Epoch 3/5\n","54000/54000 - 139s - loss: 0.1435 - accuracy: 0.9618 - val_loss: 0.1520 - val_accuracy: 0.9673 - 139s/epoch - 3ms/step\n","Epoch 4/5\n","54000/54000 - 140s - loss: 0.1387 - accuracy: 0.9648 - val_loss: 0.1498 - val_accuracy: 0.9622 - 140s/epoch - 3ms/step\n","Epoch 5/5\n","54000/54000 - 139s - loss: 0.1329 - accuracy: 0.9672 - val_loss: 0.1685 - val_accuracy: 0.9630 - 139s/epoch - 3ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7d2476acb010>"]},"metadata":{},"execution_count":5}],"source":["NUM_EPOCHS = 5\n","\n","model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"]},{"cell_type":"markdown","metadata":{"id":"oz7ixuNa2I2C"},"source":["## Test the model"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLgg1NhL2I2C","executionInfo":{"status":"ok","timestamp":1701058941424,"user_tz":300,"elapsed":1786,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}},"outputId":"b134617d-c45f-4517-e45d-9d6c018a7ef9"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 674ms/step - loss: 0.1958 - accuracy: 0.9597\n","Test loss: 0.20. Test accuracy: 95.97%\n"]}],"source":["test_loss, test_accuracy = model.evaluate(test_data)\n","print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"]},{"cell_type":"markdown","source":["\n","\n","1.   2 hidden layers, 50 layers size, act func 'relu', 'relu', batch size 100  \n","    Epoch 1/5 13s loss: 0.4045 accuracy: 0.8868 val_accuracy: 0.9408  \n","    Epoch 5/5 3s loss: 0.0910 accuracy: 0.9732 val_accuracy: 0.9717  \n","2.   2 hidden layers, 50 layers size, act func 'relu', 'relu', batch size 10000  \n","    Epoch 1/5 10s loss: 2.2362 accuracy: 0.1756 val_accuracy: 0.3723  \n","    Epoch 5/5 3s loss: 1.0924 accuracy: 0.7246 val_accuracy: 0.7882  \n","3.   2 hidden layers, 50 layers size, act func 'relu', 'relu', batch size 1  \n","    Epoch 1/5 168s loss: 0.2508 accuracy: 0.9252 val_accuracy: 0.9522  \n","    Epoch 5/5 139s loss: 0.1329 accuracy: 0.9672 val_accuracy: 0.9630  \n","\n","\n","\n"],"metadata":{"id":"ooSG-7jl4jtc"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}