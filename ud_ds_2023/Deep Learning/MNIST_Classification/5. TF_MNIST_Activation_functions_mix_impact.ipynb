{"cells":[{"cell_type":"markdown","metadata":{"id":"RyUX2eAS3HLc"},"source":["# Exercises\n","\n","### 5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string: 'tanh'\n","\n","**Solution**\n","\n","Analogically to the previous lecture, we can change the activation functions. This time though, we will u***se different activators for the different layers.***\n","\n","The ***result should not be significantly different***. However, with different width and depth, that may change.\n","\n","*Additional exercise: Try to find a better combination of activation functions*"]},{"cell_type":"markdown","source":["\n","1.   layers size 50, 2 hidden layers, act func = 'relu'  \n","    Epoch 1/5 - 16s - loss: 0.4163 - accuracy: 0.8801 val_accuracy: 0.9368  \n","    Epoch 5/5 - 3s - loss: 0.0912 - accuracy: 0.9726 val_accuracy: 0.9707\n","2.   layers size 50, 2 hidden layers, act func = 'relu' and 'tanh' mix  \n","    Epoch 1/5 12s loss: 0.4127 accuracy: 0.8851 val_accuracy: 0.9407   \n","    Epoch 5/5 3s loss: 0.0901 accuracy: 0.9730 val_accuracy: 0.9708  \n","\n"],"metadata":{"id":"60nmL3WT14L9"}},{"cell_type":"markdown","metadata":{"id":"HzPj2_uD3HLe"},"source":["# Deep Neural Network for MNIST Classification\n","\n","The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs).\n","\n","The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n","\n","The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n","\n","Our goal would be to build a neural network with 2 hidden layers."]},{"cell_type":"markdown","metadata":{"id":"qUQRSKuS3HLe"},"source":["## Import the relevant packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"PQccaLuF3HLe","executionInfo":{"status":"ok","timestamp":1701055818882,"user_tz":300,"elapsed":5134,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n"]},{"cell_type":"markdown","metadata":{"id":"snBSdPny3HLf"},"source":["## Data\n","\n","That's where we load and preprocess our data."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"r_8d9zO_3HLf","executionInfo":{"status":"ok","timestamp":1701055824712,"user_tz":300,"elapsed":5831,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["\n","mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n","\n","mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n","\n","num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n","num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n","\n","num_test_samples = mnist_info.splits['test'].num_examples\n","num_test_samples = tf.cast(num_test_samples, tf.int64)\n","\n","\n","def scale(image, label):\n","    image = tf.cast(image, tf.float32)\n","    image /= 255.\n","\n","    return image, label\n","\n","scaled_train_and_validation_data = mnist_train.map(scale)\n","test_data = mnist_test.map(scale)\n","\n","\n","BUFFER_SIZE = 10000\n","\n","shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n","validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n","train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n","\n","\n","BATCH_SIZE = 100\n","\n","train_data = train_data.batch(BATCH_SIZE)\n","validation_data = validation_data.batch(num_validation_samples)\n","test_data = test_data.batch(num_test_samples)\n","\n","validation_inputs, validation_targets = next(iter(validation_data))"]},{"cell_type":"markdown","metadata":{"id":"UjAtmePR3HLg"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"20QqZ10l3HLg"},"source":["### Outline the model\n","When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_v5MZ3Wr3HLg","executionInfo":{"status":"ok","timestamp":1701055824896,"user_tz":300,"elapsed":186,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["input_size = 784\n","output_size = 10\n","\n","hidden_layer_size = 50\n","\n","model = tf.keras.Sequential([\n","\n","    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n","\n","    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n","    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n","    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), # 2nd hidden layer\n","    # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer\n","    # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 4th hidden layer\n","    # tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 5th hidden layer\n","\n","    # the final layer is no different, we just make sure to activate it with softmax\n","    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n","])"]},{"cell_type":"markdown","metadata":{"id":"5BlrXOee3HLh"},"source":["### Choose the optimizer and the loss function"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6Wheby653HLh","executionInfo":{"status":"ok","timestamp":1701055824897,"user_tz":300,"elapsed":4,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}}},"outputs":[],"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"TaKGkcXB3HLh"},"source":["### Training\n","That's where we train the model we have built."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfCQ4PF33HLh","executionInfo":{"status":"ok","timestamp":1701055854257,"user_tz":300,"elapsed":29364,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}},"outputId":"21baa126-5d0c-48c4-d681-6c317fc08f98"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","540/540 - 12s - loss: 0.4127 - accuracy: 0.8851 - val_loss: 0.2080 - val_accuracy: 0.9407 - 12s/epoch - 23ms/step\n","Epoch 2/5\n","540/540 - 3s - loss: 0.1769 - accuracy: 0.9476 - val_loss: 0.1462 - val_accuracy: 0.9558 - 3s/epoch - 6ms/step\n","Epoch 3/5\n","540/540 - 3s - loss: 0.1304 - accuracy: 0.9615 - val_loss: 0.1245 - val_accuracy: 0.9625 - 3s/epoch - 6ms/step\n","Epoch 4/5\n","540/540 - 4s - loss: 0.1043 - accuracy: 0.9688 - val_loss: 0.1058 - val_accuracy: 0.9692 - 4s/epoch - 7ms/step\n","Epoch 5/5\n","540/540 - 3s - loss: 0.0901 - accuracy: 0.9730 - val_loss: 0.0946 - val_accuracy: 0.9708 - 3s/epoch - 6ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ad997716980>"]},"metadata":{},"execution_count":5}],"source":["NUM_EPOCHS = 5\n","\n","model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"]},{"cell_type":"markdown","metadata":{"id":"HAxSqx983HLi"},"source":["## Test the model\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxVzZEw13HLi","executionInfo":{"status":"ok","timestamp":1701055855588,"user_tz":300,"elapsed":1334,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}},"outputId":"40f69278-c5b5-4727-f800-5e403f5cda56"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 726ms/step - loss: 0.1074 - accuracy: 0.9679\n","Test loss: 0.11. Test accuracy: 96.79%\n"]}],"source":["test_loss, test_accuracy = model.evaluate(test_data)\n","print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"]},{"cell_type":"markdown","source":["\n","\n","1.   layers size 50, 2 hidden layers, act func = 'relu'  \n","    Epoch 1/5 - 16s - loss: 0.4163 - accuracy: 0.8801 val_accuracy: 0.9368  \n","    Epoch 5/5 - 3s - loss: 0.0912 - accuracy: 0.9726 val_accuracy: 0.9707\n","2.   layers size 50, 2 hidden layers, act func = 'relu' and 'tanh' mix  \n","    Epoch 1/5 12s loss: 0.4127 accuracy: 0.8851 val_accuracy: 0.9407   \n","    Epoch 5/5 3s loss: 0.0901 accuracy: 0.9730 val_accuracy: 0.9708  \n","\n","\n"],"metadata":{"id":"Oj8QIcxlzpjc"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}