{"cells":[{"cell_type":"markdown","metadata":{"id":"eGkGWYFYpfdg"},"source":["# Exercises\n","\n","### 1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n","\n","**Solution**\n","\n","Find the variable: \"hidden_layer_size\" and change it to 200.\n","\n","The **validation accuracy is significantly higher** (as the algorithm with 50 hidden units was too simple of a model).\n","\n","Naturally, it **takes the algorithm much longer to train** (**unless early stopping** is triggered too soon).\n","\n","A hidden layer size of 500 (and not only) works even better. And there is not that much increan in accuracy when the hidden layer size change to 600.\n","\n"]},{"cell_type":"markdown","source":["1.   laywers size = 50  \n","    Epoch 1/5 - 9s - loss: 0.4140 - accuracy: 0.8855 val_accuracy: 0.9377  \n","    Epoch 5/5 - 5s - loss: 0.0974 - accuracy: 0.9710 val_accuracy: 0.9698\n","2.   laywers size = 200  \n","    Epoch 1/5 - 15s - loss: 0.2747 - accuracy: 0.9208 val_accuracy: 0.9622  \n","    Epoch 5/5 - 7s - loss: 0.0400 - accuracy: 0.9872 val_accuracy: 0.9860  \n","3.   laywers size = 300  \n","    Epoch 1/5 - 15s - loss: 0.2411 - accuracy: 0.9296 val_accuracy: 0.9628  \n","    Epoch 5/5 - 6s - loss: 0.0339 - accuracy: 0.9886 val_accuracy: 0.9875  \n","4.   layers size = 400  \n","    Epoch 1/5 - 17s - loss: 0.2304 - accuracy: 0.9319 val_accuracy: 0.9683  \n","    Epoch 5/5 - 8s - loss: 0.0333 - accuracy: 0.9893 val_accuracy: 0.9888  \n","5.   layers size = 500  \n","    Epoch 1/5 - 18s - loss: 0.2175 - accuracy: 0.9355 val_accuracy: 0.9645  \n","    Epoch 5/5 - 9s - loss: 0.0291 - accuracy: 0.9905 val_accuracy: 0.9888\n","6.   layers size = 600  \n","    Epoch 1/5 - 24s - loss: 0.2138 - accuracy: 0.9357 val_accuracy: 0.9687  \n","    Epoch 5/5 - 12s - loss: 0.0299 - accuracy: 0.9903 val_accuracy: 0.9843  \n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"XRrjwiqbrkMj"}},{"cell_type":"markdown","metadata":{"id":"McTVUTwupfdj"},"source":["# Deep Neural Network for MNIST Classification\n","\n","The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs).\n","\n","The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n","\n","The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n","\n","Our goal would be to build a neural network with 2 hidden layers."]},{"cell_type":"markdown","metadata":{"id":"glSqIfxzpfdj"},"source":["## Import the relevant packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLjgBy_upfdk"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","\n","import tensorflow_datasets as tfds\n"]},{"cell_type":"markdown","metadata":{"id":"7bAxn8ugpfdl"},"source":["## Data\n","\n","That's where we load and preprocess our data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVUQ1Mq9pfdl"},"outputs":[],"source":["\n","mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n","\n","mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n","\n","num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n","num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n","\n","num_test_samples = mnist_info.splits['test'].num_examples\n","num_test_samples = tf.cast(num_test_samples, tf.int64)\n","\n","\n","def scale(image, label):\n","    image = tf.cast(image, tf.float32)\n","    image /= 255.\n","\n","    return image, label\n","\n","scaled_train_and_validation_data = mnist_train.map(scale)\n","test_data = mnist_test.map(scale)\n","\n","\n","BUFFER_SIZE = 10000\n","\n","shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n","validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n","train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n","\n","\n","BATCH_SIZE = 100\n","\n","train_data = train_data.batch(BATCH_SIZE)\n","validation_data = validation_data.batch(num_validation_samples)\n","test_data = test_data.batch(num_test_samples)\n","\n","validation_inputs, validation_targets = next(iter(validation_data))"]},{"cell_type":"markdown","metadata":{"id":"prlQ9v-9pfdl"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"fEBldeUFpfdm"},"source":["### Outline the model\n","When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"895XUdOZpfdm"},"outputs":[],"source":["input_size = 784\n","output_size = 10\n","\n","hidden_layer_size = 600\n","\n","model = tf.keras.Sequential([\n","\n","    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n","\n","    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n","    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n","\n","    # the final layer is no different, we just make sure to activate it with softmax\n","    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n","])"]},{"cell_type":"markdown","metadata":{"id":"8zESDm62pfdm"},"source":["### Choose the optimizer and the loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BekpQZJpfdn"},"outputs":[],"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"fclLiuG0pfdn"},"source":["### Training\n","That's where we train the model we have built."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E89c6vQlpfdn","executionInfo":{"status":"ok","timestamp":1701022563402,"user_tz":300,"elapsed":94443,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}},"outputId":"73dc8eee-17b4-451c-c1ea-ca02387d6a8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","540/540 - 24s - loss: 0.2138 - accuracy: 0.9357 - val_loss: 0.1006 - val_accuracy: 0.9687 - 24s/epoch - 45ms/step\n","Epoch 2/5\n","540/540 - 17s - loss: 0.0807 - accuracy: 0.9748 - val_loss: 0.0869 - val_accuracy: 0.9717 - 17s/epoch - 31ms/step\n","Epoch 3/5\n","540/540 - 15s - loss: 0.0522 - accuracy: 0.9834 - val_loss: 0.0616 - val_accuracy: 0.9795 - 15s/epoch - 28ms/step\n","Epoch 4/5\n","540/540 - 11s - loss: 0.0378 - accuracy: 0.9880 - val_loss: 0.0434 - val_accuracy: 0.9848 - 11s/epoch - 20ms/step\n","Epoch 5/5\n","540/540 - 12s - loss: 0.0299 - accuracy: 0.9903 - val_loss: 0.0498 - val_accuracy: 0.9843 - 12s/epoch - 23ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b4c089cf2e0>"]},"metadata":{},"execution_count":5}],"source":["NUM_EPOCHS = 5\n","\n","model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"]},{"cell_type":"markdown","metadata":{"id":"Cclx-CTepfdo"},"source":["## Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65gqucOTpfdo","executionInfo":{"status":"ok","timestamp":1701022564665,"user_tz":300,"elapsed":1267,"user":{"displayName":"Eugene L","userId":"13757838802534143234"}},"outputId":"0f979e09-e2b5-4927-86df-abffe2cb446d"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 1s/step - loss: 0.0768 - accuracy: 0.9791\n","Test loss: 0.08. Test accuracy: 97.91%\n"]}],"source":["test_loss, test_accuracy = model.evaluate(test_data)\n","print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"]},{"cell_type":"markdown","source":["1.   laywers size = 50  \n","    Epoch 1/5 - 9s - loss: 0.4140 - accuracy: 0.8855 val_accuracy: 0.9377  \n","    Epoch 5/5 - 5s - loss: 0.0974 - accuracy: 0.9710 val_accuracy: 0.9698\n","2.   laywers size = 200  \n","    Epoch 1/5 - 15s - loss: 0.2747 - accuracy: 0.9208 val_accuracy: 0.9622  \n","    Epoch 5/5 - 7s - loss: 0.0400 - accuracy: 0.9872 val_accuracy: 0.9860  \n","3.   laywers size = 300  \n","    Epoch 1/5 - 15s - loss: 0.2411 - accuracy: 0.9296 val_accuracy: 0.9628  \n","    Epoch 5/5 - 6s - loss: 0.0339 - accuracy: 0.9886 val_accuracy: 0.9875  \n","4.   layers size = 400  \n","    Epoch 1/5 - 17s - loss: 0.2304 - accuracy: 0.9319 val_accuracy: 0.9683  \n","    Epoch 5/5 - 8s - loss: 0.0333 - accuracy: 0.9893 val_accuracy: 0.9888  \n","5.   layers size = 500  \n","    Epoch 1/5 - 18s - loss: 0.2175 - accuracy: 0.9355 val_accuracy: 0.9645  \n","    Epoch 5/5 - 9s - loss: 0.0291 - accuracy: 0.9905 val_accuracy: 0.9888\n","6.   layers size = 600  \n","    Epoch 1/5 - 24s - loss: 0.2138 - accuracy: 0.9357 val_accuracy: 0.9687  \n","    Epoch 5/5 - 12s - loss: 0.0299 - accuracy: 0.9903 val_accuracy: 0.9843  \n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"UR9cFhXk3F5b"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}